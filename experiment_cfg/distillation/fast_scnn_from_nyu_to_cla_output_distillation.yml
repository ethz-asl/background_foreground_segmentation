# Train on CLA with output distillation using with Fast-SCNN architecture pretrained on NYU.
network_params:
  architecture: fast_scnn
  freeze_encoder: false
  model_params:
    image_h: 480
    image_w: 640
  normalization_type: batch
training_params:
  batch_size: 10
  learning_rate: 0.00001
  num_training_epochs: 100
  perform_data_augmentation: false
  reduce_lr_on_plateau: true
  stopping_min_epoch: 50
  stopping_patience: 20
  use_balanced_loss: false
dataset_params:
  replay_datasets: null
  replay_datasets_scene: null
  test_dataset: NyuDepthV2Labeled
  test_scene: null
  train_dataset: BfsegCLAMeshdistLabels
  train_scene: null
  validation_percentage: 10
logging_params:
  model_save_freq: 1
  exp_name: fast_scnn_from_nyu_to_cla_output_distillation
cl_params:
  cl_framework: distillation
  distillation_type: output
  fraction_replay_ds_to_use: null
  lambda_distillation: 0.1
  lambda_type: both_ce_and_regularization
  # Note: this needs to be specified.
  pretrained_dir: null
  ratio_main_ds_replay_ds: null